\documentclass[a4paper,11pt]{article}
\include{macros}

% \setcounter{page}{1}

\renewcommand{\baselinestretch}{1.02} 
\begin{document}


\begin{center} 
% {\large Vetenskapsr√•det: Distinguished professor grant within natural and engineering sciences 2024} \vspace{2.5mm}\\
{\Large Research plan} \vspace{3mm}\\
{\Large\bf {\proposaltitle} {\sc (}{\acronymtitle}{\sc )}}  \vspace{3mm} \\
{\Large Aristides Gionis} 
\end{center}

% \instructions{
% The research plan shall be forward-looking and consist of a brief but complete description of the research task. It may cover a maximum of 10 page-numbered A4 pages in Arial, font size 11, single line spacing and 2.5 cm margins, including references and any images. The research plan must include the following headings and information, listed in the following order:
% }

\section{Purpose and aims {\color{orange}[$\approx$1.5 pages]} {\color{teal}(@Sebastian: please make a pass)}}

% \instructions{
% State the overall purpose and specific aims of the research project.
% }

The explosive growth of artificial intelligence (AI) research and wide adoption of machine learning (ML) methods 
has revolutionized numerous facets of modern knowledge society, 
playing pivotal roles in decision making, predictive analytics, optimization tasks, and beyond. 
However, a crucial issue lies in the opacity of many existing algorithms, 
hindering understanding and accountability of machine-learning methods. 
As these methods increasingly influence critical decisions in areas such as 
healthcare, finance, and criminal justice, 
there is a growing recognition of the need for \emph{transparency}, 
\emph{interpretability}, and \emph{explainability}.
Stakeholders rightfully demand explanations for algorithmic decisions, 
as well as insights into the factors driving those decisions. 
In response to the imperative for \emph{responsible AI} and the fundamental \emph{right to explanation}, 
there has been a rising focus on developing transparent and interpretable models.

Research efforts aimed at enhancing the explainability and interpretability 
of machine-learning models and methods focus on several key directions. 
First, there is a significant emphasis on developing transparent and interpretable 
\emph{white-box models}~\cite{loyola2019black}, 
such as decision trees, rule-based systems, and linear models, 
which inherently provide insights into their decision-making processes. 
Second, a significant body of research focuses on exploring \emph{post-hoc explanation techniques}, 
including feature importance analysis~\cite{lundberg2017unified} and 
model-agnostic methods~\cite{ribeiro2016model}, 
which aim to explain predictions of complex \emph{black-box models}, 
such as deep neural networks or ensemble methods.

% chatGPT text below
% Additionally, there is a growing interest in integrating domain knowledge and human feedback into the model training process, facilitating the creation of more interpretable and trustworthy models.  Moreover, efforts are directed towards developing evaluation metrics and benchmarks  to assess the quality and reliability of explanations provided by these methods.  Finally, interdisciplinary collaboration between researchers in machine learning, cognitive science, and ethics is crucial for addressing societal concerns and ensuring the responsible deployment of explainable AI systems.

Much of research discussed in the previous paragraph focuses on \emph{supervised machine-learning methods}, 
e.g., classification, object recognition, and natural-language processing, among other.
On the other hand, significantly less attention has been given on the 
topic of explainability and interpretability for \emph{unsupervised machine learning}.
The most typical tasks in unsupervised machine learning include
\emph{data clustering}, \emph{outlier detection}, \emph{network analysis}, and more. 
In this project we will focus on the problem of \emph{explainable clustering}, 
although some of our ideas can be also extended to other tasks.

Clustering is a fundamental task in data analysis and machine learning, 
seeking to partition data into groups based on their similarity.
Clustering algorithms enable data scientists and practitioners
to uncover hidden patterns, extract meaningful insights, 
and make informed decisions without the need for labeled data. 
Clustering plays a pivotal role in various domains
% , including  document analysis, bioinformatics, and recommendation systems, 
while it also serves as a preprocessing step for other machine-learning tasks, 
helping to streamline feature engineering, visualization, and model~building.

Although hugely popular, 
traditional clustering methods often lack transparency in explaining 
why certain data points are grouped together, 
thus necessitating a manual validation process to make sense of the clustering. 
As the data complexity keeps growing, 
it becomes increasingly hard to understand the reasons behind a clustering.
While many clustering methods rely on optimizing simple-to-state objective functions, 
the resulting clusterings are not always simple to explain. 
Furthermore, a large number of clustering methods use opaque approaches
for which explanations are not available. 
Examples include density-based approaches, spectral techniques, embedding methods,  
agglomerative strategies, self-organizing maps, and~more.

Recognizing the need for transparent and explainable clustering methods, 
researchers have recently proposed ideas and techniques for finding 
interpretable reasoning behind clusters.
While a review of those methods is presented in the next section, 
most notable examples include 
explaining a black-box clustering using decision trees~\cite{gupta2023price,moshkovitz2020explainable},
descriptive labels~\cite{davidson2018cluster,sambaturu2020efficient}, 
\textcolor{red}{or \ldots}.

\spara{Challenges and limitations.}
While these methods present many novel ideas and make significant advances, 
many challenges remain and there is ample space for fruitful research in this important topic. 
In particular, existing methods fail to integrate with the most common variants of kernel-based clustering, 
thus failing to yield explainable clusters on models exploiting non-linear similarities.
Second, existing methods do not cater to real-world data challenges where data has special features, 
labels, or covariates, as they cannot distinguish between special covariate features and data features.
Additionally, out of the large toolbox of explainable machine-learning models,
decision trees have primarily been used for explanations, 
while other models could be better suited for certain applications. 
Last but not least, while the majority of existing methods focus on explaining black-box clusterings, 
it is often important to \emph{a priori} cluster the data so that the result complies 
by-design with a family of explainable models.
Our objective in this project is to address many of these challenges
and make significance advances in the topic of explainable clustering.

\medskip
\noindent
\hspace{-3mm}\colorbox{verylightmagenta}{
\begin{minipage}{\textwidth}
{\bf High-level goal of \acronym:} 
We will develop novel theoretical frameworks for 
introducing transparency and explainablity into the task of data clustering. 
We will develop explainable clustering methods that employ novel white-box models, 
and we will devise methods for providing explanations to clusterings produced by complex clustering models.
Furthermore, we will study formulations that leverage special features of the data,
such as labels and covariates.
\end{minipage}}

\mpara{Research objectives.}
To achieve our objective of advancing the state of the art in explainable clustering
we aim to consolidate existing approaches,  including our own work,  
introduce novel abstractions, 
develope rigorous computational methods, and 
perform evaluations on real-world datasets.
In particular, {\acronym} has the following research objectives. 

\vspace{-2mm}
\begin{description}
\setlength{\itemsep}{-4pt}
\item[{Models and problems:}]
Develop novel models and novel problem formulations that enable 
obtaining a deeper understanding of the problem of explainable clustering.
Focus on three themes: 
\emph{novel explainable models}, 
\emph{explainability beyond centroid-based clustering}, and 
\emph{explaining clustering with covariates}.

\item[{Algorithms:}]
Develop computational methods for the problems that will be formulated.
Our methods will rely on different techniques
including combinatorial optimization, approximation algorithms, and linear algebra.
The proposed algorithms should be efficient and should offer theoretical guarantees.

\item[{Empirical evaluation and outreach:}]
Implement the proposed methods and evaluate them on 
real-world benchmark datasets from different application areas.
Collaborate with experts in other areas for applying the methods in their domain.
% Validate proof-of-concept by showcasing findings of the methods on different use cases. 
Make the implementation of our methods available to the scientific community.

\item[{Education in KTH:}]
Support a new doctoral student in KTH, 
educate them on algorithmic data analysis and responsible AI. 
Create an environment of collaboration for the new doctoral student 
with postdocs in the group of the PI, working on the same topic, 
as well as with other researchers in the international collaboration network of the PI.
\end{description}

\vspace{-2mm}
\noindent
We next discuss briefly the state of the art
before presenting the proposed approach.

\section{State of the art {\color{orange}[$\approx$1 page]} {\color{teal}(Sebastian)}}

% \instructions{
% Summarize briefly the current research frontier within the field or area covered by the project. State key references.
% }

Algorithmically grouping similar data points has a long research history,
yielding traditional methods (such as $k$-means~\cite{todo}, DBSCAN~\cite{todo}, or Spectral Clustering~\cite{todo}).
Classical clustering algorithms only insufficiently provide descriptions why data points are deemed similar, 
for example, by referring to dense data regions or the closeness to a data centroid.

As scientists, data analysts, or companies~\footnote{\note{cite the law that requires companies to explain why algorithms made a user-centric decision}} have to know true reasons beind a clustering, 
the structures within groups, or how to tell clusters apart; 
they turn to a comprehensive, manual, subjective, and often visualization-based~\cite{todo} post-hoc analysis.
However, with the advent of complex, large-scale, and high-dimensional datasets, 
clustering descriptors increasingly become harder to interpret. 
Addressing this problem, Explainable Clustering seeks to not only group similar data points but also tells us 
why there is a cluster in the data,
why it assigned a data point to a cluster, or 
how to tell groups apart.

Addressing the complexity interpreting clusterings of high-dimensional data has conventionally been done with \textbf{dimensionality reduction} or subspace techniques, which condense the original data into a lower-dimensional representation, while aiming to preserve relationships between data points. 
Notable techniques such as Non-negative Matrix Factorization (NMF), t-distributed stochastic neighbor embedding (t-SNE), Uniform Manifold Approximation and Projection (UMAP), Auto-encoders (AE), Principal Component Analysis (PCA), or independent component analysis (ICA) are commonly employed. 
Dimensionality reduction, is sometimes used as a pre-processing step for clustering to manually enhance interpretability.
However, using different data representations present challenges in terms of interpretability, as they produce complex models that may not readily elucidate underlying data structures and does not reliably result in explainable clusters.
Recent advancements directly integrate clustering with dimensional NMF~\cite{}, but notably require a kernelized data representation, thus do not offer any explainability. 
Retaining the original feature space, thus enhancing the interpretability,
\textbf{Subspace clusterings} seek to select the most salient subset of features that are solely responsible for a clustering, either as a post-processing step~\cite{todo} or during clustering~\cite{todo}.
Although improving interpretability, a subspace clustering of complex data often requires numerous features, thus failing to clearly explain the reasons behind clusters~\cite{todo}. 

Keeping all features during clustering, \textbf{feature importance measures} are used to assign an importance to features after the clustering is done, 
thereby aiding the manual assessments of the clustering,
aiming to justify the hardly interpretable output of traditional clustering algorithms.   
Traditional cluster-specific feature importance measures include impurity or Gini coefficients~\cite{todo}, statistical testing~\cite{todo}, or 
Recent developments include Shapley values via SHapley Additive exPlanations (SHAP) or its variants~\cite{todo}.
However, they do not exactly describe the mechanisms that ultimately lead to a cluster assignment, 
thus leaving room for subjective interpretability.

A subset of research on explainable clustering methods aims to identify the reasons behind a clustering through a set of feature-based decisions that almost mechanistically leads to the result. 
Considerable attention has been given to rule-based explanations using decision trees. 
Recent advancements have focused on enhancing the theoretically attainable performance~\cite{todo,todo,todo} or improving interpretability by integrating decision trees directly into the clustering algorithm~\cite{}. 
However, it has been demonstrated that concise explanations with small decision trees are generally unattainable if data complexity increases, 
thereby necessitating explanations in terms of on more powerful models like oblique decision trees \cite{} or kernelized features spaces \cite{}.
However, to obtain interpretable rules that explain clusterings with features, decision trees require access to the original feature space, prohibiting kernelized clustering algorithms and their ability to exploit nonlinearities, thus potentially reducing clustering quality.
Research on explaining clusterings that utilize dimensionality reduction techniques or feature mappings are still in its infancy.

\section{Significance and scientific novelty  {\color{orange}[$\approx$0.5 page]} {\color{teal}(Aris)}}

\instructions{
Describe briefly how the project relates to previous research within the area, and the impact the project may have in the short and long term. Describe also how the project moves forward or innovates the current research frontier.
}

\section{Preliminary and previous results  {\color{orange}[$\approx$0.5 page]} {\color{teal}(@Sebastian: please make a pass)}}

% \instructions{
% Describe briefly your own previous research and pilot studies within the research area that make it probable that the project will be feasible. If no preliminary results exist, please state this too. State also whether the project contributes further to research and scientific results from a grant awarded previously by the Swedish Research Council.
% }

The PI is in a unique position to accomplish the goals of the project. 
His profile brings together theoretical work on algorithmic data analysis, 
with development of practical data-mining methods, and emphasis on applications. 
While the PI has not worked directly on the problem of explainable clustering,
he has published numerous high-impact articles on other related computational tasks, 
including work on data clustering and explainable machine learning. 

With respect to clustering, 
the earlier work of the PI includes papers on
\emph{correlation clustering}~\cite{bonchi2013overlapping,gionis2007clustering},
unified framework for \emph{clustering with outlier detection}~\cite{chawla2013k},
\emph{kernelized clustering}~\cite{amid2015kernel}, and more.
Most recent work includes papers on
\emph{diversity-aware clustering}~\cite{thejaswi2021diversity}, 
which is a formulation asking for cluster centers being diverse with respect to covariate attributes, and
\emph{reconciliation clustering}~\cite{spoerhase2023constant}, 
which is a formulation asking to avoid selecting cluster centers that are far away to each other.
None of these problems provides an explainable clustering, 
thus, it would be interesting to study whether we can introduce explainability
into these formulations.
%
With respect to explainable machine learning, 
the PI has published papers on 
\emph{explainable time-series classification with counterfactual analysis}~\cite{karlsson2020locally},
\emph{explainable rule-based classification models}~\cite{ciaperoni2023concise,zhang2020diverse}
and 
\emph{explainable decision-tree models}~\cite{zhang2023regularized}. 
Related to this project is also the work
finding dense subgraphs with covariate explanations~\cite{galbrun2014overlapping,galbrun2016top}.
We believe that ideas and techniques from that line of work
will be very relevant to the third research theme of this project.

A member of the research team of the PI is postdoctoral researcher Sebastian Dalleiger, 
who will contribute to this project by supervising the recruited PhD student.
Dr.\ Dalleiger has expertise on many aspects of theoretical machine learning, 
including highly relevant work on \emph{explainable data decompositions}~\cite{dalleiger2020explainable}.

\section{Project description {\color{orange}[$\approx$5 pages]}}

\instructions{
Describe the project design under the following headings:
}

\subsection{Theory and methods {\color{orange}[$\approx$3-4 pages]} {\color{teal}(Sebastian)}}

\instructions{
Describe the underlying theory and the methods to be applied in order to reach the project goal.
}

The project is structured along three {\em research themes}:

\begin{description}
\setlength{\itemsep}{-2pt}
\item[\rto.~\newmodels\,:] 

\item[\rtw.~\clusterings\,:] 

\item[\rth.~\covariates\,:]
\end{description}

When considering different approaches for explainable clustering we distinguish between 
two different computational paradigms. 
\begin{description}
\setlength{\itemsep}{-2pt}
\item[\posthoc\,:]
In the first paradigm, we assume that a clustering of the data is provided 
from a \emph{black-box clustering method} and the problem objective is to design 
a \emph{post-hoc} explanation of the input clustering.
This task is similar to \emph{interpretable machine learning}~\cite{XXX}, 
where one asks to develop post-hoc interpretable justifications for 
\emph{black-box classification models}, such as neural networks. 
The difference is \ldots 
\item[\joint\,:]
In the second paradigm, one is given as input a set of data points, 
and the goal is to find an optimal clustering of the data
from a family of explainable clustering models.
That is, one needs to \emph{cluster the data} and \emph{select the explainable clustering model}
in a \emph{joint fashion}.
In the general case, the clustering cost incurred by an explainable model 
will be larger than the cost incurred by an optimal (but non-explainable) model, 
so our goal will be to bound the cost of the explainable model with respect to the optimal cost.
\end{description}

Next we discuss in more detail the three research themes of \acronym.
The two paradigms, \posthoc\ and \joint, are present in all three of our research themes.
When discussing different explainable-clustering approaches 
we specify the paradigm for a specific approach.

\subsection*{Research theme 1 (\rto): \newmodels\ {\color{orange}[$\approx$1 page]} {\color{teal}(Sebastian)}}

Given a clustering produced by any black-box clustering method, e.g., $k$-means,
provide explanations of the clustering using models that go beyond axis-aligned decision trees.
Such models include 
(i) non-axis aligned decision trees;
(ii) decision rules or decision lists;
(iii) ??
Here we can also include work on explaining clusterings using feature importance. 

Existing methods that explain clustering results with axis-aligned decision trees
provide quality guarantees.
Ensuring that our methods, will provide quality guarantees for the novel models
we will introduce is a major challenge for this research theme.

\subsection*{Research theme 2 (\rtw): \clusterings\ {\color{orange}[$\approx$1 page]} {\color{teal}(Sebastian)}}

Here we will focus on developing methods providing explanations for clusterings 
that go beyond centroid-based approaches like $k$-means or $k$-median.
In particular we will consider the following clustering approaches.
\begin{itemize}
\setlength{\itemsep}{-2pt}
\item kernel-based clustering
\item density-based clustering; what would be a good model for this case?
\item subspace clustering \hfill\ag{does this make sense?}
\item diversity-aware (or fair) clustering \hfill\ag{does this make sense?}
\item what if want the explanation be fair, instead of the clustering?
\item clusterings with embeddings
\end{itemize}

There is strong interaction between \rto and \rtw, 
due to the fact that providing explanations for clusterings that go beyond centroid-based methods
will require working with novel models.

\subsection*{Research theme 3 (\rth): \covariates\ {\color{orange}[$\approx$1 page]} {\color{teal}(@Sebastian: please make a pass)}}

In many application scenarios data can be represented in different ways: 
Most often, data can be represented in a vector space
in which appropriate distance functions, or similarity functions, are considered.
Such a vector space can be defined using directly data features, 
or it can be learned using representation-learning techniques~\cite{hamilton2017representation,wang2020survey}.
Second, data points are associated with covariates, 
which are discrete (categorical) attributes, 
representing certain aspects of the data. 
Such covariates can be labels, tags, or other related information.
Given the discrete nature of covariates it is typically not easy (or meaningful)
to define distance functions based on them, 
although sometimes coarse-grained distance functions can be specified.

For an example illustrating the setting described above, 
consider a collection of news articles. 
Each news article can be represented by a vector in a high-dimensional space, 
e.g., using bag-of-words, \tfidf-type weighting, or deep-learning embedding schemes. 
Furthermore, for each news article we can associate a small 
set of tags, or labels, summarizing the topic of the article, 
e.g., an article reporting on climate change could be labeled by
`\texttt{\small ClimateCrisis}' and `\texttt{\small ExtremeWeather}.'
We refer to these labels as covariate~attributes.

When considering the problem of clustering a dataset in which the data points are 
represented both in vector space and with covariate attributes, 
it is immediate clear how to leverage the vectorial information
using a standard clustering method, such as $k$-means or spectral clustering. 
One of the other hand, it is not obvious how to use effectively the 
covariate attributes in order to improve the clustering results. 

In this research theme we propose finding high-quality clusterings of the data
while using the covariate attributes for explaining the resulting clusterings. 
In particular, we aim to develop novel clustering methods
that achieve low-cost solutions with respect to the vector-representation of the data points, 
while at the same time achieve high expressivity and high specificity of the clusterings
using the covariate attributes. 

To be more concrete, as one possible problem formulation that we will explore, 
let us consider a set of $n$ data points $\dataset=\{\vecx_1,\ldots,\vecx_n\}$
and a set of covariate attributes {\labelset} representing labels for the data points. 
Each data point $\vecx_i$ is associated with a set of labels $\labels_i\subseteq\labelset$. 
A set of labels $\labels=\{\alabel_1,\ldots,\alabel_k\}\subseteq\labelset$, 
henceforth referred to as \emph{descriptor set}, 
\emph{induces} a subset of the data points $\dataset(\labels)\subseteq\dataset$
via a \emph{logic grammar}, such as, for example, 
$\dataset(\labels)$ consists of the data points in \dataset that 
are associated with at least $s$ labels contained in \labels.
For the set of points $\dataset(\labels)$ induced by \labels 
we can then define its \emph{clustering cost} $d(\dataset(\labels))$, 
using the geometry of the points in the set, 
e.g., the diameter of the set, or the sum of distances of all points to the median of the set.
The problem of explainable clustering with covariate attributes that we will study is the following: 
find a collection of label sets $\labels_1,\ldots,\labels_t$, 
so that the induced clusters $\dataset(\labels_1),\ldots,\dataset(\labels_t)$
cover the whole dataset \dataset, 
and some aggregate function on the cost of the clusters is minimized. 
Natural choices for the total clustering cost are
$d_{+}(\labels_1,\ldots,\labels_t) = \sum_{j} d(\dataset(\labels_j))$ and 
$d_{m}(\labels_1,\ldots,\labels_t) = \max_{j}\! \left\{ d(\dataset(\labels_j)) \right\}$, 
that is, we will aim to minimize the total cost, or worst-case cost, respectively, 
among all clusters. 

A few observations are in order for the problem that we define above.
First we note that the problem belongs in the \joint paradigm, 
that is, we aim to \emph{simultaneously} find a low-cost clustering, 
according to vector representation of the data points,
and an explanation of that clustering using the covariate attributes~(labels).
%
Second, notice that we, in fact, define a very large family of problems, 
depending on the logic grammar used to induce clusters from label sets
and the distance-based function used to define the clustering cost.
Notable logic grammars to consider are the disjunctive and conjunctive grammars 
(i.e., a cluster $\dataset(\labels)$ consists of the points 
that contains at least one, or all, respectively, labels in \labels).
Our goal is to systematically study the computational complexity and 
develop methods with provable guarantees
for many of these possible~formulations.
%
Third, we note that in our discussion above we do not impose any
\emph{disjointedness constraint} with respect to label sets or 
memberships of data points to clusters. 
In fact, such constraints are not necessary as minimizing 
our distance-based objectives has the consequence of \emph{implicitly favoring}
disjoint labels and disjoint clusters.
Nevertheless it would be interesting to study variants
of our problem with explicitly stated disjointedness constraints
and understand, theoretically and empirically, 
the price in the clustering cost due to such constraints.

The covariate-based explainability problem we propose in this theme
has interesting connections with existing work, yet it poses novel research challenges.
The most closely-related work is the \emph{cluster descriptor} problem
proposed by Davidson et al.~\cite{davidson2018cluster} 
and studied in follow-up recent papers~\citep{sambaturu2020efficient}.
The main difference is that those earlier works belong in the \posthoc paradigm, 
that is, the clustering is provided as input. 
Our \joint-type formulation provides a more holistic approach and a
significantly better way of leveraging the covariate attributes into the explanation of clustering. 
It also poses significantly different computational challenges
and the opportunity to exploit the geometric structure for solving the problem.

Depending on the progress on this topic, further extensions of the problem can be considered. 
Those include allowing for outliers that can help reducing significantly the clustering cost.
The benefit of this idea is that covariate attributes can be used in the same manner
to also explain the outliers, as the clusters.
Additionally, we can use the same framework analyzing networks and finding 
explainable communities in networks.
The PI has relevant experience of developing methods for finding labeled-induced
(and thus, explainable) densest subgraphs in networks~\cite{galbrun2014overlapping,galbrun2016top}.

\subsection{Time plan and implementation {\color{orange}[$\approx$0.75-1 page]} {\color{teal}(Aris)}}

\instructions{
Describe summarily the time plan for the project during the grant period, and how the project will be implemented. Describe also any crucial risks or obstacles that may impact on the implementation, and your plan for managing these.
}

\begin{figure}[t]
\begin{center}
\vspace{-2mm}
{\small\input{gantt}}
\end{center}
\vspace{-6mm}
\caption{\label{figure:gantt}The time schedule of {\acronym}.}
\vspace{-2mm}
\end{figure}

\subsection{Project organization {\color{orange}[$\approx$0.5 page]} {\color{teal}(Aris)}}

\instructions{
Clarify how you and any participating researchers will contribute to the implementation of the project. Explain in particular how the time allocated by you (that is, your activity level) as project leader is suitable for the task, including the relationship with your other research undertakings. Describe and explain the competences and roles of the participating researchers in the project, and also other key persons (including any doctoral students) who are important for the implementation of the project.
}

\section{Equipment {\color{orange}[$\approx$0.2 page]} {\color{teal}(Aris)}}

\instructions{
Describe the basic equipment you and your team have at your disposal for the project.
}

\section{Need for research infrastructure {\color{orange}[$\approx$0.2 page]} {\color{teal}(Aris)}}

\instructions{
Specify the project‚Äôs need for international and national research infrastructure. If you choose to use other infrastructure than those supported by the Swedish Research Council External link.and that are thereby open to all, you must justify this (also applies to local research infrastructure).
}

The project is mainly of theoretical nature and will not require extensive computing infrastructure. 
Commodity laptops will be provided to all team members. 
For implementing and evaluating our methods we will use the available 
KTH computing facilities
and the National Academic Infrastructure for Supercomputing in Sweden (NAISS).

\section{International and national collaboration {\color{orange}[$\approx$0.2 page]} {\color{teal}(Aris)}}

\instructions{
Describe your own and the team‚Äôs collaboration with foreign and Swedish researchers and research teams. State whether you contribute to or refer to international collaboration in your research.
}

The PI has an extensive international collaboration network. 
Recent and on-going collaborations include
prof.\ De Bie in Ghent University, 
prof.\ Terzi in Boston University,
prof.\ Mannila in Aalto University, and 
Dr.\ Bonchi in Centai Labs.
In spring 2024 the PI will spent one month as a visiting professor 
in Sapienza University of Rome, hosted by prof.\ Leonardi.
In the near future the PI will apply for a sabbatical in Stanford University, 
planning to visit prof.\ Ugander. 
We will seek to strengthen and further expand this collaboration network.
We will encourage the research team to be actively involved in national and international collaborations
and make research visits and internships in other institutes.

\section{Independent line of research {\color{orange}[$\approx$0.2 page]} {\color{teal}(Aris)}}

\instructions{
If you are working or will be working in a larger group, please clarify how your project relates to the other projects in the group. If you are (continuing) working in the same team as your doctoral or postdoc supervisor, or if you are continuing a project that wholly or partly started during your doctoral or postdoc studies, you must also describe the relationship between your project and the research of your former supervisor.
}



{\small
\setlength{\bibsep}{0pt}
\bibliographystyle{abbrv}
\bibliography{references}
}

{\color{orange}References 1 page maximum} 

% \newpage
% \input{rebound}

\end{document}




