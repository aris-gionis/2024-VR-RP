\documentclass[a4paper,11pt]{article}
\include{macros}

% \setcounter{page}{1}

\renewcommand{\baselinestretch}{1.02} 
\begin{document}


\begin{center} 
% {\large Vetenskapsr√•det: Distinguished professor grant within natural and engineering sciences 2024} \vspace{2.5mm}\\
{\Large Research plan} \vspace{3mm}\\
{\Large\bf {\proposaltitle} {\sc (}{\acronymtitle}{\sc )}}  \vspace{3mm} \\
{\Large Aristides Gionis} 
\end{center}

% \instructions{
% The research plan shall be forward-looking and consist of a brief but complete description of the research task. It may cover a maximum of 10 page-numbered A4 pages in Arial, font size 11, single line spacing and 2.5 cm margins, including references and any images. The research plan must include the following headings and information, listed in the following order:
% }

\subsection*{1~~~Purpose and aims}

% \instructions{
% State the overall purpose and specific aims of the research project.
% }

The explosive growth of artificial intelligence (AI) research and wide adoption of machine learning (ML) methods 
has revolutionized numerous facets of modern knowledge society, 
playing pivotal roles in decision-making, predictive analytics, optimization tasks, and beyond. 
However, a crucial issue lies in the opacity of many existing algorithms, 
hindering understanding and accountability of machine-learning methods. 
As these methods increasingly influence critical decisions in areas such as 
healthcare, finance, and criminal justice, 
there is a growing recognition of the need for \emph{transparency}, 
\emph{interpretability}, and \emph{explainability}.
Stakeholders rightfully demand explanations for algorithmic decisions, 
as well as insights into the factors driving those decisions. 
In response to the imperative for \emph{responsible AI} and the fundamental \emph{right to~expla\-na\-tion},%
\!\footnote{\url{https://commission.europa.eu/law/law-topic/data-protection/data-protection-eu_en}}
there has been an increasing focus on developing transparent and interpretable models.

Research efforts aimed at enhancing the explainability and interpretability 
of machine-learning models and methods focus on several key directions. 
First, there is a significant emphasis on developing transparent and interpretable 
\emph{white-box models}~\cite{loyola2019black}, 
such as decision trees, rule-based systems, and linear models, 
which inherently provide insights into their decision-making processes. 
Second, a large body of research has been devoted on developing \emph{post-hoc explanation techniques}, 
including feature importance analysis~\cite{lundberg2017unified} and 
model-agnostic methods~\cite{ribeiro2016model}, 
which aim to explain predictions of complex \emph{black-box models}, 
such as deep neural networks or ensemble methods.

% chatGPT text below
% Additionally, there is a growing interest in integrating domain knowledge and human feedback into the model training process, facilitating the creation of more interpretable and trustworthy models.  Moreover, efforts are directed towards developing evaluation metrics and benchmarks  to assess the quality and reliability of explanations provided by these methods.  Finally, interdisciplinary collaboration between researchers in machine learning, cognitive science, and ethics is crucial for addressing societal concerns and ensuring the responsible deployment of explainable AI systems.

Much of the research discussed in the previous paragraph focuses on \emph{supervised machine-learning methods}, 
such as classification, object recognition, and natural-language processing.
On the other hand, significantly less attention has been given on the 
topic of explainability and interpretability for \emph{unsupervised machine learning}.
Typical tasks in unsupervised machine learning include
\emph{data clustering}, \emph{outlier detection}, \emph{network analysis}, among others. 
In this proposal we focus on the problem of \emph{explainable clustering}, 
although some of our ideas can be also extended to other tasks.

Clustering is a fundamental task in data analysis and machine learning, 
seeking to partition data into groups based on their similarity.
Clustering algorithms enable data scientists and practitioners
to uncover hidden patterns, extract meaningful insights, 
and make informed decisions without the need for labeled data. 
Clustering plays a pivotal role in various domains, 
including  document analysis, bioinformatics, and recommendation systems, 
while it also serves as a preprocessing step for other machine-learning tasks, 
helping to streamline feature engineering, visualization, and model~building.

Although hugely popular, 
traditional clustering methods often lack transparency in explaining 
why certain data points are grouped together, 
thus necessitating a manual validation process to make sense of the clustering. 
As the data complexity keeps growing, 
it becomes increasingly hard to understand the reasons behind a clustering.
While many clustering methods rely on optimizing simple-to-state objective functions, 
the resulting clusterings are not always simple to explain. 
Furthermore, numerous clustering methods use opaque approaches
for which explanations are not yet available. 
Examples include density-based approaches, spectral techniques, embedding methods,  
agglomerative strategies, self-organizing maps, and~more.

Recognizing the need for transparent and explainable clustering methods, 
researchers have recently proposed ideas and techniques for finding 
interpretable reasoning behind clusters.
Notable examples include 
explaining a black-box clustering using decision trees~\cite{gupta2023price,moshkovitz2020explainable},
or descriptive labels~\cite{davidson2018cluster,sambaturu2020efficient}, 
while a more thorough review of this line of research is presented in the next section.

\spara{Challenges and limitations.}
While existing explainable clustering methods present many novel ideas and make significant advances, 
many challenges remain and there is ample space for fruitful research in this important topic. 
In particular, existing methods fail to integrate with the most common variants of kernel-based clustering, 
thus failing to yield explainable clusters on models exploiting non-linear similarities.
Second, existing methods do not cater to real-world data challenges where data has special features, 
labels, or covariates, as they cannot distinguish between special covariate features and data features.
Additionally, out of the large toolbox of explainable machine-learning models,
decision trees have primarily been used for explanations, 
while other models could be better suited for certain applications. 
Last but not least, while the majority of existing methods focus on explaining black-box clusterings, 
it is often important to \emph{a priori} cluster the data so that the result complies 
\emph{by-design} with a family of explainable models.
Our objective in this project is to address many of these challenges
and make significance advances in the topic of explainable clustering.

\medskip
\noindent
\hspace{-3mm}\colorbox{verylightmagenta}{
\begin{minipage}{\textwidth}
{\bf High-level goal of \acronym:} 
We will develop novel theoretical frameworks for 
introducing transparency and explainability into the task of data clustering. 
We will develop explainable clustering methods that employ novel white-box models, 
and we will devise methods for providing explanations to clusterings produced by complex clustering models.
Furthermore, we will study formulations that leverage special features of the data,
such as labels and covariates.
\end{minipage}}

\mpara{Research objectives.}
To achieve our objective of advancing the state of the art in explainable clustering,
we aim to consolidate existing approaches, including our own work,  
introduce novel abstractions, 
develop rigorous computational methods, and 
perform evaluations on real-world datasets.
In particular, {\acronym} has the following research objectives. 

\vspace{-2mm}
\begin{description}
\setlength{\itemsep}{-4pt}
\item[{Models and problems:}]
Develop novel models and novel problem formulations that enable 
obtaining a deeper understanding of the problem of explainable clustering.
Focus on three themes: 
\emph{novel explainable models}, 
\emph{explainability beyond centroid-based clustering}, and 
\emph{explaining clustering with covariates}.

\item[{Algorithms:}]
Develop computational methods for the problems that will be formulated.
Our methods will rely on different techniques
including combinatorial optimization and statistical learning theory.
The proposed algorithms should be efficient, practical, and should offer theoretical guarantees.

\item[{Empirical evaluation and outreach:}]
Implement the proposed methods and evaluate them on 
real-world benchmark datasets from different application areas.
Collaborate with experts in other areas for applying our methods in their domain.
% Validate proof-of-concept by showcasing findings of the methods on different use cases. 
Make the implementation of our methods publicly available to the scientific community.

\item[{Education in KTH:}]
Support a new doctoral student in KTH, 
educate them on the topic of algorithmic data analysis and responsible AI. 
Create a collaborative environment between the new doctoral student 
our postdocs working on the same topic, 
and other researchers in the international collaboration-network of the PI.
\end{description}

\iffalse
\begin{figure} 
    \centering
    \includegraphics[height=1.8in]{iris-exclus.pdf}
    \caption{\small Explaining $k$-means clusters through axis-aligned decision trees. First clustering the iris dataset with $k$-means, we then employ an axis-aligned decision tree as explanation, yielding interpretable description of clusters while resulting in a minimal quality loss. Left: Visualization of the decision boundaries. Right: Representation of the decision tree explanation.}
    \label{fig:iris}
\end{figure}
\fi

\vspace{-2mm}
\noindent
Next, we briefly discuss the state of the art in explainable clustering, before proposing our research approach.

\subsection*{2~~~State of the art}

% \instructions{
% Summarize briefly the current research frontier within the field or area covered by the project. State key references.
% }

Designing algorithms for grouping similar data points has a long research history,
yielding traditional methods, 
such as $k$-means~\cite{macqueen1967some}, 
DBSCAN~\cite{ester1996ada}, or spectral clustering~\cite{ng2001spectral}.
Methods to enhance cluster interpretability have long been focused on subspace clustering~\cite{rubinstein2010dict}, 
% (such as $k$-SVD~\cite{rubinstein2010dict}), 
feature-importance measures, such as Shapley values \cite{lundberg2017unified}, 
or dimensionality reduction~\cite{hinton2002tsne,mcinnes2018umap}.
% (such as $t$-SNE~\cite{hinton2002tsne}, UMAP~\cite{mcinnes2018umap}, or NMF~\cite{lee2000algorithms}).
From the sprawling explainable machine-learning research, the area of explainable or interpretable clustering has emerged, aiming to rationalize the underlying reasons for clusters. 
Our proposal is situated in this dynamic and active research area. 
% whose state-of-the-art we will sketch in the following. 

Considerable attention has been given to \emph{explaining clusters} with decision trees,
including explaining using similarity-based prototypes for each cluster \cite{carrizosa2022interpreting}, 
or by fitting oblique trees to describe clusters \cite{gabidolla2022optimal}.
In their seminal work, Moshkovitz et al.~\cite{moshkovitz2020explainable} 
explain $k$-means and $k$-medians with axis-aligned decision trees, 
deriving a worst-case approximation guarantee, termed as ``price of explainability,'' 
inspired by the ``price of interpretability'' \cite{bertsimas2019price} for axis-aligned decision tree classification. 
Recent developments focus on improving worst-case bounds on explainability for $k$-means, 
by using random threshold cuts for axis-aligned decisions 
% \cite{makarychev2022explainable, esfandiari2022almost, makarychev2023random}. 
\cite{esfandiari2022almost, makarychev2022explainable}. 
The best known upper bound on the price of explainability for $k$-means is 
$\mathcal{O}(k \log \log k)$ \cite{gupta2023price}, 
with a known lower bound of $\Omega(k)$ \cite{gamlath2021explainable}.

As narrow the gap already is, 
it can further be improved for low-dimensional problems \cite{charikar2022near,laber2021price}. 
Aiming for a slight generalization, the restriction that decision trees have to have $k$ leaves has been removed, improving the worst-case guarantees \cite{makarychev2022explainable} and 
showing good performance in practice \cite{frost2020exkmc}. 
Now free of this restriction trees can grow larger.
As such growth inhibits interpretability, 
Laber et al.~\cite{laber2023nearly} explore shallow decision trees for explainable clustering, 
while Deng et al.~\cite{deng2023impossibility} demonstrate 
that shallow trees for explainable $k$-means and $k$-medians are impossible to achieve in general. 
Further research includes explainability for $k$-centers~\cite{laber2021price} or 
explainable clustering under $\|\cdot\|_p$-norms \cite{gamlath2021explainable}. 
Research on explaining kernel $k$-means has only recently been started, 
showing that axis-aligned trees lead to approximation guarantees for a limited set of interpretable kernels, 
while showing an \emph{unbounded} ``price of explainability'' 
for explaining the widely-used quadratic kernel \cite{fleissner2024explaining}.

In summary, recent advances focus on explaining (mostly) $k$-means clusters with (mostly) axis-aligned trees. 
Research on more expressive explanation like oblique decision trees or complex clustering 
like kernel $k$-means is still in its infancy.

Closely related to our proposal is research in the area of \emph{interpretable clustering},
seeking to compute inherently interpretable clusters \emph{without} resorting to post-hoc explanations.
Recent research has focused on aiding clustering towards interpretable solutions by directly integrating 
decision-tree explanations into the clustering algorithm.
One approach leverages \emph{mixed integer linear programming} 
to derive an interpretable tree-based clustering algorithm~\cite{bertsimas2021interpretable}. 
% by approximating the globally-optimal solution~\cite{bertsimas2021interpretable}. 
% Also building on combinatorial optimization, 
Another approach increases the expressivity while decreasing interpretability, 
by leveraging soft decision trees~\cite{cohen2023interpretable} as a guide for $k$-means. 
A different take on explainability or interpretability for clusterings 
moves away from explaining centroids or data points 
but describes clusters boundaries with poly\-hedra, 
again using combinatorial optimization~\cite{lawless2023polyhedral}. 

% Some clustering algorithms aim to find interpretability fro embeddings via \textbf{dimensionality reduction} algorithms such as nonnegative matrix factorization \cite{} or tensor decompositions \cite{}.    
% Related to our research themes are inherently \textbf{interpretable machine learning} methods,
% employable as substitutes for the decision tree models, for example 

% Classic approaches that are related to explainable clustering include \textbf{subspace clustering} which aims at discovering clusters in high-dimensional data by assuming that data points only belong to certain lower-dimensional subspace clusters rather than being globally clustered, thus enhancing interpretability.
% While many subspace methods such as $k$-SVD~\cite{} focus on subspace selection, explanations emphasizes understanding the rationale behind clusters, which often in practice is elucidated with feature importance analysis.
% While selecting the top most important features for clustering  
% An important research area lies in measuring and ranking \textbf{feature importance} for clusters.   
% In the context of explainable clustering, measuring the feature importance falls into two categories:
% \emph{unsupervised} if no clusters are given (\joint) and \emph{supervised} if clusters are given (\posthoc).


\subsection*{3~~~Significance and scientific novelty}

% \instructions{
% Describe briefly how the project relates to previous research within the area, and the impact the project may have in the short and long term. Describe also how the project moves forward or innovates the current research frontier.
% }

\para{Significance.} Our project lies in the intersection of the areas of  
algorithms design, knowledge discovery, and explainable machine learning.
The problem domain we propose to study, 
explainability in unsupervised machine learning and data clustering, 
will enable domain scientists and practitioners to better understand their data.
Not only will they discover meaningful clusters, 
but explainable clustering algorithms will also elucidate the fundamental mechanisms in the creation of clusters, 
the importance and contribution of features, and ways to explain the clusters
with covariate attributes.

\spara{Scientific novelty.} 
The project will make contributions in several different directions
all related to trustworthy, transparent, and explainable machine learning.
The research area of trustworthy, transparent, and explainable machine learning 
has gained a lot of traction in recent years, but most attention
has been given to supervised machine learning.
While explainability for unsupervised machine learning---and in particular in data clustering---% 
is not completely unexplored, 
the research area is still at its infancy.
%  and most of the works fall within a few frameworks. 

Here we propose to significantly enlarge the scope of research in explainable unsupervised machine learning with a focus on clustering. 
Specifically, we seek to explore many novel directions in terms of
different interpretable machine-learning models that explain a given clustering result, 
explainability for complex clustering that goes beyond centroids, and 
discovering optimal clusters in conjunction with short explanations using covariate attributes. 
Our work will be foundational in nature, 
focusing on methods that provide formal guarantees, 
and leveraging ideas from combinatorial optimization, 
information theory, probabilistic algorithms, and statistical learning theory.

\subsection*{4~~~Preliminary and previous results}

% \instructions{
% Describe briefly your own previous research and pilot studies within the research area that make it probable that the project will be feasible. If no preliminary results exist, please state this too. State also whether the project contributes further to research and scientific results from a grant awarded previously by the Swedish Research Council.
% }

The PI is in a unique position to accomplish the goals of the project. 
His profile brings together theoretical work on algorithmic data analysis, 
with development of practical data-mining methods, and emphasis on applications. 
While the PI has not worked directly on the problem of explainable clustering,
he has published numerous high-impact articles on other related computational tasks, 
including work on data clustering and explainable machine learning. 

With respect to clustering, 
the earlier work of the PI includes papers on
\emph{correlation clustering}~\cite{bonchi2013overlapping,gionis2007clustering},
unified framework for \emph{clustering with outlier detection}~\cite{chawla2013k},
\emph{kernelized clustering}~\cite{amid2015kernel}, and more.
Most recent work includes papers on
\emph{diversity-aware clustering}~\cite{thejaswi2021diversity}, 
which is a formulation asking for cluster centers being diverse with respect to covariate attributes, and
\emph{reconciliation clustering}~\cite{spoerhase2023constant}, 
which is a formulation asking to avoid selecting cluster centers that are far away to each other.
None of these approaches provides an explainable clustering, 
thus, it would be interesting to study whether we can introduce explainability
into these formulations.
%
With respect to explainable machine learning, 
the PI has published papers on 
\emph{explainable time-series classification with counterfactual analysis}~\cite{karlsson2020locally},
\emph{explainable rule-based classification models}~\cite{ciaperoni2023concise,zhang2020diverse}
and 
\emph{explainable decision-tree models}~\cite{zhang2023regularized}. 
Related to this project is also the work
finding dense subgraphs with covariate explanations~\cite{galbrun2014overlapping}.
We believe that ideas and techniques from that line of work
will be very relevant to the third research theme of this project.

A member of the research team of the PI is postdoctoral researcher Sebastian Dalleiger, 
who will contribute to this project by supervising the recruited PhD student.
Dr.\ Dalleiger has expertise on many aspects of theoretical machine learning, 
including highly relevant work on \emph{explainable data decompositions}~\cite{dalleiger2020explainable}
statistically \emph{significant feature interactions}~\cite{dalleiger2022spass}, 
or \emph{matrix factorization}~\cite{dalleiger2022efficiently}.

\subsection*{5~~~Project description}

%\instructions{
%Describe the project design under the following headings:
%}

\subsection*{5.1~~~Theory and methods}

%\instructions{
%Describe the underlying theory and the methods to be applied in order to reach the project goal.
%}

The project is structured along three {\em research themes}:

\vspace{-2mm}
\begin{description}
\setlength{\itemsep}{-2pt}
\item[\rto.~\newmodels\,:] 
Explain clustering using a richer family of interpretable machine-learning models, 
including oblique decision trees, Bayesian decision rules, cascading rule lists, and 
generalized linear models. 
Develop methods to finding interpretable cluster explanations 
via highlighting the importance of features and the feature interactions for each cluster.

\item[\rtw.~\clusterings\,:] 
Develop methods to explain more complex clustering structure than centroid-based approaches,
for example, clustering that have resulted by kernel $k$-means or density-based formulations.

\item[\rth.~\covariates\,:]
Develop methods for obtaining high-quality clusterings of data 
while using the covariate attributes for explaining the resulting clusterings. 
In this theme we aim to minimize clustering cost, based on vector-based representations, 
while achieving high expressivity and high specificity of the clusterings, 
based on covariate attributes.

\end{description}

\vspace{-2mm}
When considering different approaches for explainable clustering we distinguish between 
two different computational paradigms. 

% \vspace{-2mm}
% \begin{description}
% \setlength{\itemsep}{-2pt}

\smallskip
% \noindent
{\posthoc\,:}
In the first paradigm, we assume that a clustering of the data is provided 
by a \emph{black-box clustering method} and the problem objective is to design 
a \emph{post-hoc} explanation of the input clustering.
This task is similar to \emph{interpretable machine learning}, 
where one asks to develop post-hoc justifications for 
\emph{black-box classification models}, such as self-organizing maps. 

\smallskip
% \noindent
{\joint\,:}
In the second paradigm the goal is to find an optimal clustering of the data
from a family of explainable clustering models.
That is, one needs to \emph{cluster the data} and \emph{select the explainable clustering model}
in a \emph{joint manner}.
In the general case, the clustering cost incurred by an inherently interpretable model 
will be larger than the cost incurred by an optimal (but non-explainable) model, 
so our goal will be to bound the cost of the interpretable model with respect to the optimal cost.
% \end{description}

\smallskip
Next we discuss in more detail the three research themes of \acronym.
We present formulations both in \posthoc\ and \joint paradigms.

% The two paradigms, \posthoc\ and \joint, are present in all three of our research themes.
% When discussing different explainable-clustering approaches 
% we specify the paradigm for a specific approach.

\subsection*{Research theme 1 (\rto): \newmodels}

At the core of many data-analysis questions lies the task of discovering clusters in data. 
In many cases we are interested not only in what are the prominent clusters, 
but also which mechanisms are responsible for them,  
what are their characteristics, and how to tell them apart.  
A life scientist, for example, may want to determine the factors that tell apart 
high-risk from low-risk clusters, 
or may seek to highlight the characteristics that are specific to a disease cluster.  
% How can we explain clusters with sets of specific features with particularly observed values?

A broadly-used \posthoc approach in explaining \emph{pre-existing} $k$-means clusters uses 
\emph{axis-aligned decision trees}. 
That is, to attribute data to clusters, 
we only have to mechanistically answer a sequence of yes-no questions, 
which have been algorithmically derived from data features, 
thereby describing clusters with easily understandable and inherently interpretable axis-aligned rules (cuts), 
such as ``\texttt{if} $x_i \in [\textnormal{lb}, \textnormal{ub}]$ \texttt{then} 
proceed with left sub\-tree \texttt{else} proceed with right sub\-tree \texttt{end} until a cluster at a leaf is reached.''
Reaching clusters by means of such an interpretable model incurs 
a cost in addition to the original $k$-means clustering cost, 
which is called \emph{the price of explainability}.
Central to our research is the question of how large the price to pay becomes, i.e., 
what is the worst-case achievable approximation guarantee of expressing a clustering 
in terms of a specific interpretable machine-learning model. 

Although the price for explaining $k$-means with axis-aligned decision tree might be low~\cite{gupta2023price}, 
a recent investigation shows that the cost lies in the size of the tree~\cite{deng2023impossibility}.
Simply put, a precise explanation requires large trees.
The larger the clustering complexity, the larger the explanations, thereby 
% shrinking interpretability to a point where 
the benefits of having an explanation over analyzing raw data diminishes.
The bottom line is: there is more to explainability than guaranteeing a clustering quality under interpretable rules.
A core challenge of explainable clustering is to identify how to properly quantify explainability,
what are good explanations, and how well do they address real-world circumstances in theory and practice.
We propose to holistically explore new kinds of explanations to enhance explainable clustering,
by advancing decision trees, exploring rule-based explanations, and by investigating important features for clusters.

First, we plan to explore advanced decision tree models with more expressive individual decision function per node, aiming to guarantee concise explanations even when considering complex data.
The principle is simple: the more expressive individual decision functions are, the fewer decisions we need to make. 
One avenue leads to oblique decision trees, whose decision functions generalize simple axis-aligned decision-making to hyperplane-based decision-making, akin to $\mathbf{a}_i^{\!\top} \mathbf{x}^{\vphantom{\top}}_{\vphantom{i}} + b_i$. % < 0
The danger of employing expressive decision functions, however, lies in the increased complexity of each individual decisions functions, thus negatively impacting individual interpretability.
Therefore, a core challenge of this research theme is the question of how to find expressive functions that balance individual interpretability, overall explainability, and accuracy.
In the context of hyperplanes $\mathbf{a}_i^{\!\top} \mathbf{x}^{\vphantom{\top}}_{\vphantom{i}} + b_i < c_i$, for example, we seek to reduce the complexity of individual hyperplanes using non-negativity constraints $\mathbf{a} \in \mathbb{R}^m_{\geq 0}$ and sparsity \emph{constraints} $\|\mathbf{a}\|_0 \leq q$, by employing combinatorial optimization techniques for small problems or (greedy) subspace algorithms such as orthogonal matching pursuit or sequential lasso.
%  or top-$q$ important feature forward selection for large problems.

An alternative approach is to go beyond decision trees to explore other models capable of explaining clusters. 
Machine-learning models such as Bayesian decision rules, cascading rule lists, and generalized linear models 
are prime candidates to explore, given their widespread adoption and inherent interpretability. 
One concrete goal is to study how we can explain clusters with concise decision rule lists 
of the form ``\texttt{if} $\bigwedge_i \llbracket x_i \in [\textnormal{lb}, \textnormal{ub}] \rrbracket$ 
\texttt{then} cluster$_a$ \texttt{else} proceed with the next rule \texttt{end}.'' 

By exploiting the additional modeling flexibility, 
we specifically aim to investigate explainable clustering for \emph{knowledge-seeking applications}, 
where an under-researched problem is that (rule-based) explanations 
often repeat already-known salient signals, thus failing to convey new knowledge.
For example, 
% if the algorithm accurately explains a cluster `\texttt{\small ClimateCrisis}' in terms of a high concentration of `\texttt{\small CO\textsubscript{2}}', it would fail to tell us about `\texttt{\small methane}'; or 
an explanation of `\texttt{\small Diabetes}' with a highly-expressed 
but known-to-be-relevant gene `\texttt{\small TCF7L2}', 
might hide an almost equally relevant gene `\texttt{\small ZNT8}'.  
As the algorithm does not know in advance which explanations are most informative to the user, 
we aim to provide sets of explanations that offer \emph{diverse perspectives} on the clustering.
In the context of rule sets $\mathcal{R} = \{r_1, \dots, r_n\}$, 
we seek to explain a clustering $Y$ of data points $X$ 
by minimizing the loss $\ell(\mathcal{R},Y;X)$ between explained clustering and the estimated clustering 
penalized by the rule diversity~$d(\mathcal{R})$, 
that is, minimizing $\ell(\mathcal{R},Y;X) + \lambda \operatorname{d}(\mathcal{R})$~\cite{zhang2020diverse}.
% \ag{Do you want to include more details, e.g., about \textsc{MaxSum} diversification?}
So far, diverse explanations for clustering is an understudied subject, and quality, conciseness, and empirical performance are unknown. 
We aspire to change this by studying the theoretical underpinning, the price of diversity for explainability clustering, its practical implication on clustering accuracy, and the ability to acquire new knowledge. 

A major challenge of finding interpretable explanations is in judging the importance of data features per cluster.
Having an importance measure allows us to explain clusters in terms of importance `spectra' such as $\{\slifI,\slifII,\slifIII\}$, thereby highlighting informative feature-values that are characteristic, contrastive, or common in clusters.
% In this example, we notice that feature $1$ and $2$ are significantly more important to cluster $b$ than $a$ or $c$.
Although importance measures such as Shapely values or impurity typically consider features independently, 
in reality, features often interact with each other.
We hypothesize that features interact differently per cluster and that differences in those interactions are relevant reasons for specific clusters, as evidenced in our prior work~\cite{dalleiger2020explainable}.
%
We plan to investigate novel higher-order importance measures to highlight the characteristics or contrastive per-cluster feature-interactions, leveraging ideas from information theory, statistics, and statistical learning.
However, research on the importance of higher-order feature interactions for data clusters is still lacking, 
as this poses significant algorithmic and statistical challenges, by involving exponentially many potential interactions for which non-linear or multi-variate analytics is necessary to compute importance.  
% We plan to explore novel measures to judge the importance of higher-order feature interactions, leveraging ideas from information theory, statistics, and statistical learning, thus highlighting sets of important feature-interactions that are relevant for clusters.

\iffalse
\note{Solution details? Should we remove this section?}

Regardless of expressivity, any \posthoc explanation has to explain \emph{pre-existing} clusters of almost arbitrary complexity.
Colloquially speaking, that is because the clustering algorithm does not know that we want explainability.
How can we make clustering aware of the explainability of its result?
An important aspect of \rto is the question of how to incorporate explanations into clustering as a \joint problem.
For classical $k$-means, for example, one obvious objective $\arg\;\min_{e, w, c} \sum_j \sum_i P_{ki}(e) w_{ki} \| x_i - c_j \|$ augments the $k$-means objective with a probability or weight $P_{ki}$ that our explanation $e$ concludes that data point $x_i$ is in cluster $j$. 
In case that $E$ is an oblique decision tree, for example, the probability $P_{ki}$ stems from a sequence of hyperplane-derived probabilities.
We hope to explore the cost that joining explanations with clustering would inflict on the clustering performance, computational complexity, and explainability of the results.
\fi 

% \note{

% Central to our research is the concept of the price of explainability, as the worst-case achievable approximation guarantee of expressing a clustering in terms of a different interpretable machine learning model.

% Given a clustering produced by any black-box clustering method, e.g., $k$-means,
% provide explanations of the clustering using models that go beyond axis-aligned decision trees.
% Such models include 
% (i) non-axis aligned decision trees;
% (ii) decision rules or decision lists;
% (iii) ??
% Here we can also include work on explaining clusterings using feature importance. 

% Existing methods that explain clustering results with axis-aligned decision trees
% provide quality guarantees.
% Ensuring that our methods, will provide quality guarantees for the novel models
% we will introduce is a major challenge for this research theme.

\subsection*{Research theme 2 (\rtw): \clusterings}

Most recent research on explainable clustering has focused on classical $k$-means,
thereby resulting in algorithms that approximate $k$-means clusters with axis-aligned decision trees.
However, the classical $k$-means algorithm has limited applicability in practice, as it is often not flexible enough to handle complex datasets that contain complicated cluster shapes.
So far, the price of explainability in the context of other algorithms is understudied and questions such as ``how well axis-aligned decision trees explain DBSCAN in the worst case?''\ or\ ``how well do oblique decision trees approximate spectral-clustering~or hier\-archical-clustering results?''\ remain unanswered.
We are interested in complex clusters that are neither identifiable with axis-aligned cuts nor are linearly separable,
therefore we want we go beyond~classical $k$-means towards developing explaining clustering algorithms for complex clusters.

Arguably one of the most inhibiting factors is $k$-means inability to exploit non-linearities in the relationship between data, which is often addressed by kernelized $k$-means algorithms.
Explaining kernel $k$-means, however, poses significant challenges, as clusters are harder to split apart 
with simple axis-aligned decision trees. 
In fact, a very recent advancement has discovered that the worst-case price of explaining quadratic-kernel-based clusterings with axis-aligned trees is \emph{theoretically unbounded}~\cite{fleissner2024explaining}, 
i.e., explanations can be arbitrarily bad. 
This raises some of our core challenges in \rtw:
\textbf{(1)} are there different explainable clustering algorithms that achieves tighter bounds hard-to-explain kernels?;
\textbf{(2)} what are inherently interpretable kernels for explainable clustering?; and
\textbf{(3)} would more expressivity reduce the price of explaining complex clusters (cf., \rto)?
% \textbf{(4)} Going beyond worst-case analysis, what are the \emph{expected} or the \emph{lower bound} to the price of explainability?; and
% \textbf{(5)} Can we align approximation guarantees to the geometries of explanations and clusterings?

We like to explore the limits of simple explanations for complex clusters.  
Although post-hoc explanations might become arbitrarily bad for complex clusters, we do not yet know how well jointly explaining and clustering performs.
That is, we want to raise the awareness of clustering algorithms for simple explanations, thus steering-away from hard-to-explain complex clusters towards simpler cases.
This involves balancing clustering performance, explainability, and interpretability as a unified objective function for explainability and clustering.
Solving for optimal clustering and optimal explanations (e.g., via trees) naively, however, requires joining two likely-to-be \NP-hard problems into one.
As this is not practical for real-world problems, we propose to develop novel efficient algorithms, aiming for relaxations of the joint mathematical optimization problem.
So far, even for $k$-means there is only limited research on jointly clustering and explaining. 
It remains unclear how to combine $k$-means with different explanations, such as decision rule lists or characteristic importance measures. 

A common property of many clustering algorithms lies in their ability to separate non-linearly separable clusters using a different data representation, often involving the kernel trick.
That means explainable clustering would cluster data points in one representation while we want to explain clusters in terms of their original representation.
While post-hoc explanations for clusters might work in practice (although it might yield deep trees),
how to jointly cluster and explain two different data representations is not clear.
During optimization, we need to explain why data points are close in one representation using features from the original representation, 
allowing for interpretable explanations in principle.
Even if we postulate rules from the original features during optimization, we cannot directly derive the true reasons why data points are clustered.  
One challenge of this research theme is to explore how to align different representations during optimization so that we can achieve inherently interpretable explainable clustering algorithms.
However, the mathematical properties that allow to project data into a different representation that enables explainability and separability, are mostly unknown.
In recent research~\cite{fleissner2024explaining} on explainable kernel $k$-means, interpretable kernels using interpretable feature maps, where each component in the kernel depends on exactly a single data feature, has been introduced.  
This severe limitation rules out commonly-used kernels, such as the Gaussian kernel, and is a significant obstacle for using and advancing explainable kernel $k$-means and explainable spectral clustering.
To overcome this issue, we propose to explore a less limited framework for interpretable kernel feature maps, allowing to attribute distances or clustering decisions to the original feature representation. 
% It might, however, require to relax on the axis-aligned descriptions by considering richer and more expressive explanations.   

% Implications on spectral clustering

% Next, we like to explore \emph{expressive explanations for complex clusters}.  
% That is, what are the easierst explanaitions for (non-linearly sepaerable) complex clusters?
% As there are many different explanaitions that we like to consider in \rto, 
% we plan to develop a more generalized approach to 

\iffalse
% research on  clustering algorithms with Explanations in terms of rule lists or importance measures are  

% A different idea takes us to inherently interpretable kernel feature maps, 
% seeking maps that allow us to trace similarities back to the original data features. 
% We plan to explore suitably transparent kernels $\langle \phi(x), \phi(y) \rangle$ that allow 
% for attributing cluster-decisions to features, 
% by identifying inherently-interpretable kernel feature maps $\phi(x)$. 
% The traceability might necessitate the development of novel algorithms that cluster via the kernel while extracting rules by leveraging the interpretability of the kernel. 

% More generally, we see a lot of potential for developing new algorithms for interpretable kernelized clustering that steer-away from
% clusters that are identifiable by our kernel but are hardly explainable in terms of less-expressive human-understandable models (such as axis-aligned decision trees), thereby allowing us to balance explainability with the power of kernel clustering.
In a similar spirit, we will explore the idea of using multiple kernels 
in which clustering is performed based on one kernel (e.g., the quadratic kernel), 
while explanations come from inherently interpretable kernels.

Next we seek to study \emph{expressive explanations for complex clusters}.
For example, can guarantee a finite price for explaining geometric-kernel-based clustering with more-expressive sparse oblique trees?
Further, in practice, we often observe cases that the majority points in clustered data are, generally speaking, well separable.
Few data points at the boundaries, however, create rough edges in between clusters.
Precisely explaining those borders with simple expressions is costly.
If humans, on the other hand, have to explain this situation, they would probably cut corners, so to speak.
Focusing on the most important salient parts of the clustering, they would simply ignore the hard-to-explain corner cases.
How can we algorithmically identify the parts of a cluster that we want to explain and the parts that require complex explanations?

More concretely, we want to deliberately allow imperfect descriptions for complicated situations,
for example, by turning to 

% Building on research from \rto, 
% Can we, for example, explain 
% For example, if a border between clusters is a sinusoidal curve, we would probably require many axis-aligned `step' functions to separate clusters. 
% A more expressive, yet easily understandable description would be a simple function. 
% However, what if the border is not smooth and not easily explainable?
Next, humans are willing to use simpler approximations.
Similarly, we would like to explore fuzzy explanations 
that may provide imperfect description of complicated border situations.
More concretely, can we find fuzzy or Bayesian rule sets that explain clusters with high probability, 
however with simpler terms?
We seek to study if these explanations are suitable to explain soft clustering, 
like fuzzy $c$-means, and how to develop a \joint explainable fuzzy clustering algorithm, 
which at the moment is an understudied research area.
The theoretical challenge lies in exploring if and how to adapt the price of explainability 
for imperfect and uncertain explanations. 

One potential avenue towards explainable kernelized clustering involves inherently interpretable `surrogates.'
That is, can we approximate potentially hardly-interpretable projections with simpler estimations?
Inspired by interpretable machine learning, we like to explore explainable machine-learning models that 
\emph{learn to predict how distant} data points are to one another, 
leveraging research from explainable regression (e.g., via regression tree variants) and pairwise comparison problems.
In a similar vain, we are interested in approximating kernel feature maps with interpretable embeddings. 
A core challenge involves the definition, estimation, or learning of novel explainable surrogate functions, 
or maps, to achieve a distance measure allowing for complex clusters while being strictly interpretable.
% In the area of \emph{algorithms with predictions}, we then plan to derive an algorithm for explainable clustering with predicted distances, thereby building on the interpretability of the distance matrices.

Furthermore, we hypothesize that the price of explainability differs across geometries or topologies, 
such as hyperbolic versus Euclidean spaces, 
combining ideas from geometric machine learning for explainability. 
That is, we seek to explore if specifically-structured observations (e.g., tree-like data) 
that require exponentially-sized models in Euclidean space, 
can be described in polynomial number of rules in a more appropriate (e.g., hyperbolic) space.
Sometimes, we can perfectly separate clusters in the data with only a single axis-aligned decision.
For example, an apple-shaped cluster is on the left of the $y$-axis and a banana-shaped cluster is on its right. 
The $y$-axis, however, would fail do fully explain how the clusters are different.
Inspired by robust clustering and polyhedral descriptions \cite{lawless2023polyhedral},
we hope to explore more complex geometric yet interpretable descriptions of complex clusters.
% seeking to generalize polyhedral, study shape-interpretability. 
We additionally plan to investigate approximation guarantees for complex kernel clusters and seek to develop a geometric explainable clustering algorithm.
\fi

% Here we will focus on developing methods providing explanations for clusterings 
% that go beyond centroid-based approaches like $k$-means or $k$-median.
% In particular we will consider the following clustering approaches.
% \begin{itemize}
% \setlength{\itemsep}{-2pt}
% \item kernel-based clustering
% \item density-based clustering; what would be a good model for this case?
% \item subspace clustering \hfill\ag{does this make sense?}
% \item diversity-aware (or fair) clustering \hfill\ag{does this make sense?}
% \item what if want the explanation be fair, instead of the clustering?
% \item clusterings with embeddings
% \end{itemize}

% There is strong interaction between \rto and \rtw, 
% due to the fact that providing explanations for clusterings that go beyond centroid-based methods
% will require working with novel models.

\subsection*{Research theme 3 (\rth): \covariates}

In many applications, data can be represented in more than one way: 
Often, data can be represented in a vector space
accompanied with appropriate distance or similarity functions.
Such a vector space can be defined using directly data features, 
or it can be learned using representation-learning techniques~\cite{hamilton2017representation,wang2020survey}.
Second, data points are associated with covariates, 
which are discrete (categorical) attributes, 
representing certain aspects of the data. 
Such covariates can be labels, tags, or other related information.
Given the discrete nature of covariates it is typically not easy (or meaningful)
to define distance functions based on them, 
although sometimes coarse-grained distance functions can be specified.

For an example illustrating the setting described above, 
consider a collection of news articles. 
Each news article can be represented by a vector in a high-dimensional space, 
e.g., using bag-of-words, \tfidf-type weighting, or deep-learning embedding schemes. 
Furthermore, each news article can be associated with a small 
set of tags, or labels, summarizing the topic of the article, 
e.g., an article on climate change could be labeled by
`\texttt{\small ClimateCrisis}' and `\texttt{\small ExtremeWeather}.'
We refer to these labels as covariates.

When considering clustering data that are represented both as vectors and with covariates, 
it is usually clear how to leverage the vectorial information
and apply a standard clustering method, such as $k$-means or spectral clustering. 
One of the other hand, it is not obvious how to use effectively the 
covariate attributes in order to improve the clustering results. 

In this research theme we propose finding high-quality clusterings of the data
while using the covariates for explaining the resulting clusterings. 
In particular, we aim to develop novel clustering methods
that achieve low-cost solutions with respect to the vector-representation of the data, 
while at the same time achieve high expressivity and high specificity of the clustering
using the covariates. 

To be more concrete, as one possible problem formulation that we will explore, 
let us consider a set of $n$ data points $\dataset=\{\vecx_1,\ldots,\vecx_n\}$
and a set of covariates {\labelset} representing labels for the data points. 
Each data point $\vecx_i$ is associated with a set of labels $\labels_i\subseteq\labelset$. 
A set of labels $\labels=\{\alabel_1,\ldots,\alabel_k\}\subseteq\labelset$, 
henceforth referred to as \emph{descriptor set}, 
\emph{induces} a subset of the data points $\dataset(\labels)\subseteq\dataset$
via a \emph{logic grammar}, for example,  
$\dataset(\labels)$ consists of the data points in \dataset that 
are associated with at least $s$ labels contained in \labels.
For the set of points $\dataset(\labels)$ induced by \labels 
we can then define its \emph{clustering cost} $d(\dataset(\labels))$, 
using the geometry of the points in the set, 
e.g., the diameter of the set, or the sum of distances of all points to the median of the set.
The problem of explainable clustering with covariate attributes that we will study is the following: 
find a collection of label sets $\labels_1,\ldots,\labels_t$, 
so that the induced clusters $\dataset(\labels_1),\ldots,\dataset(\labels_t)$
cover the whole dataset \dataset, 
and some aggregate function on the cost of the clusters is minimized. 
Natural choices for the total clustering cost are
$d_{+}(\labels_1,\ldots,\labels_t) = \sum_{j} d(\dataset(\labels_j))$ and 
$d_{m}(\labels_1,\ldots,\labels_t) = \max_{j}\! \left\{ d(\dataset(\labels_j)) \right\}$, 
that is, we will aim to minimize the total cost, or worst-case cost, respectively, 
among all clusters. 

A few observations are in order for the problem defined above.
First we note that the problem belongs in the \joint paradigm, 
that is, we aim to \emph{simultaneously} find a low-cost clustering, 
according to vector representation of the data points,
and an explanation of that clustering using the covariate attributes~(labels).
%
Second, notice that we, in fact, define a very large family of problems, 
depending on the logic grammar used to induce clusters from label sets
and the distance-based function used to define the clustering cost.
Notable logic grammars to consider are the disjunctive and conjunctive grammars 
(i.e., a cluster $\dataset(\labels)$ consists of the points 
that contains at least one, or all, respectively, labels in \labels).
Our goal is to systematically study the computational complexity and 
develop methods with provable guarantees
for many of these possible~formulations.
%
Third, we note that in our discussion above we do not impose any
\emph{disjointedness constraint} with respect to label sets or 
memberships of data points to clusters. 
In fact, such constraints are not necessary as minimizing 
our distance-based objectives has the consequence of \emph{implicitly favoring}
disjoint labels and disjoint clusters.
Nevertheless, it would be interesting to study variants
of our problem with explicitly stated disjointedness constraints
and understand, theoretically and empirically, 
the price in the clustering cost due to such constraints.

The covariate-based explainability problem we propose in this theme
has interesting connections with existing work, yet it poses novel research challenges.
The most closely-related work is the \emph{cluster descriptor} problem
proposed by Davidson et al.~\cite{davidson2018cluster} 
and studied in follow-up recent papers~\citep{sambaturu2020efficient}.
The main difference is that those earlier works belong in the \posthoc paradigm, 
that is, the clustering is provided as input. 
Our \joint-type formulation provides a more holistic approach and a
significantly better way of leveraging the covariate attributes into the explanation of clustering. 
It also poses significantly different computational challenges
and the opportunity to exploit the geometric structure for solving the problem.

Depending on the progress on this topic, further extensions of the problem can be considered. 
Those include allowing for outliers, which can help to significantly reduce the clustering cost
and improve explainability.
In other words, covariates can be used in the same manner
to explain the both the clusters and the outliers.
Additionally, we can apply the same framework to analyze network structure and find
explainable communities in networks.
The PI has relevant experience of developing methods for finding labeled-induced
(and thus, explainable) densest subgraphs in networks~\cite{galbrun2014overlapping}.

\subsection*{5.2~~~Time plan and implementation}

% \instructions{
% Describe summarily the time plan for the project during the grant period, and how the project will be implemented. Describe also any crucial risks or obstacles that may impact on the implementation, and your plan for managing these.
% }

\begin{figure}[t]
\begin{center}
\vspace{-2mm}
{\small\input{gantt}}
\end{center}
\vspace{-6mm}
\caption{\label{figure:gantt}The time schedule of {\acronym}.}
\vspace{-2mm}
\end{figure}

The time schedule of \acronym is depicted in Figure~\ref{figure:gantt}. 
The diagram shows the duration of the research themes over the lifetime of the project
and the personnel working on the project.

The funding by the Swedish Research Council will be used to recruit 
one doctoral student who will work exclusively on this project, 
in addition to their teaching duties in KTH.
The doctoral student will be funded at 80\% level from this project, 
and the remaining 20\% will be provided from KTH for the student's teaching duties. 
At this level of research activity, the expected completion time of the doctoral student is 5 years. 
The fifth year, after the completion of the project, 
will be provided by KTH basic funding allocation to the PI, or other external funding source. 

The PI will allocate 12\% of his time on this project.
In addition, two postdoctoral researchers in the research team of the PI
will work on the project at 50\% of their time. 
The time of the postdocs will be funded by the WASP recruitment package of the PI, 
thus, increasing the added value of this project.
The first postdoctoral researcher is Sebastian Dalleiger, 
who is already a member of the research team, 
while the second postdoc will be recruited in the second year of the project.

In terms of the work allocation on the research themes, 
the PI will work on all research themes and will supervise the work of the other team members. 
The doctoral student will work on research themes \#1 and \#2, 
as those are the most closely related and there are many interesting research questions
and ample potential for new methods to develop that fulfill the requirements for a doctoral dissertation. 
The postdocs, together with the PI, will focus on carrying out research theme \#3,
while also helping to supervise the doctoral student on themes \#1 and \#2.

\spara{Research output.}
We aim publishing our work at top-tier international venues, 
focusing on quality rather than quantity.
% 
% in the areas of Knowledge Discovery, Machine Learning, and Artificial Intelligence.
% We will focus on the quality of publications rather than quantity.
Target journals 
% for disseminating our work 
are IEEE TKDE, PVLDB, ACM TKDD,  DMKD, etc. 
Target conferences are NeurIPS, ICML, ICLR, SIGMOD, SIGKDD, WebConf, WSDM, etc.

\spara{Materials.}
Our publications will be available via open access in {\small\url{arxiv.org}}. 
The software and the other outputs of the project 
will become freely available to the scientific community via {\small\url{github.com}}. % and archived in {\small\url{zenoodo.org}}.

\spara{Risks and mitigation.}
Devising efficient algorithms with provable quality guarantees is a challenging task
and the largest risk of the project. 
Achieving this objective, however, offers the largest potential for scientific impact.
%  in the computer science community. 
If we are not able to prove theoretical results, 
we will study problems with simplifying assumptions, and 
will focus on devising heuristic methods and providing empirical validation.
Another risk is recruiting strong team members for the project. 
To mitigate this risk, we will leverage our international network
and the excellent reputation of the research group and research environment in Sweden. 
All the members of the project will be also affiliated with the WASP program, 
which will provide further incentive for recruitment and opportunities 
for network and collaborations during the project.


\subsection*{5.3~~~Project organization}

% \instructions{
% Clarify how you and any participating researchers will contribute to the implementation of the project. Explain in particular how the time allocated by you (that is, your activity level) as project leader is suitable for the task, including the relationship with your other research undertakings. Describe and explain the competences and roles of the participating researchers in the project, and also other key persons (including any doctoral students) who are important for the implementation of the project.
% }

The PI will devote 12\% of his time in the project. 
He will supervise the doctoral student and two postdocs, 
be responsible for scientific lead, 
and will allocate time to work on mathematical and algorithmic problems.
He will be responsible for ensuring flow of information and collaborations
with other groups in KTH and his international collaboration network, 
offering the possibility to the group to make research visits and internships.
The PI is currently managing an ERC Advanced Grant (2020 to 2026),
which employs 3 doctoral students and 3 postdocs, 
so the ERC project will be ending during the first year of this project. 
In addition, the PI is still utilizing resources from his WASP recruitment package, 
which are not tied to any specific research project, 
and as explained in the previous section part of those resources will be allocated to the {\acronym} project.

% The doctoral students and postdocs will work in pairs on the corresponding research theme.
For the work in research team, 
we will encourage an environment of openness and collaboration, 
while ensuring that each team member leads their own project.
%
During the hiring process we will support diversity and consider actions to achieve gender balance. 
Currently, the PI supervises 6 doctoral students and the gender ratio is 3:3.


% \subsection*{Equipment {\color{orange}[$\approx$0.2 page]} {\color{teal}(Aris)}}

% \instructions{
% Describe the basic equipment you and your team have at your disposal for the project.
% }

\subsection*{6~~~Equipment and need for research infrastructure}

% \instructions{
% Specify the project‚Äôs need for international and national research infrastructure. If you choose to use other infrastructure than those supported by the Swedish Research Council External link.and that are thereby open to all, you must justify this (also applies to local research infrastructure).
% }

The project is mainly of theoretical nature and will not require extensive computing infrastructure. 
Commodity laptops will be provided to all team members. 
For implementing and evaluating our methods we will use the available 
KTH computing facilities
and the National Academic Infrastructure for Supercomputing in Sweden (NAISS).
The team has also access to the Berzelius supercomputer, 
available to WASP-affiliated faculty, 
which is among the world's 100 fastest AI supercomputers.

\subsection*{7~~~International and national collaboration}

% \instructions{
% Describe your own and the team‚Äôs collaboration with foreign and Swedish researchers and research teams. State whether you contribute to or refer to international collaboration in your research.
% }

The PI has an extensive international collaboration network. 
Recent and ongoing collaborations include
prof.\ De Bie in Ghent University, 
prof.\ Terzi in Boston University,
prof.\ Mannila in Aalto University, and 
Dr.\ Bonchi in Centai Labs.
In spring 2024 the PI will spend one month as a visiting professor 
in Sapienza University of Rome, hosted by prof.\ Leonardi.
In the near future the PI will apply for a sabbatical in Stanford University, 
planning to visit prof.\ Ugander. 
We will seek to strengthen and further expand this collaboration network.
We will encourage the research team to be actively involved in national and international collaborations
and make research visits and internships in other institutes.

\subsection*{8~~~Independent line of research}

% \instructions{
% If you are working or will be working in a larger group, please clarify how your project relates to the other projects in the group. If you are (continuing) working in the same team as your doctoral or postdoc supervisor, or if you are continuing a project that wholly or partly started during your doctoral or postdoc studies, you must also describe the relationship between your project and the research of your former supervisor.
% }

This is a new line of research in our group. 
We have recently worked on data clustering~\cite{spoerhase2023constant,thejaswi2021diversity}
and on explainable supervised-learning~\cite{ciaperoni2023concise,zhang2020diverse,zhang2023regularized}, 
but research in explainable unsupervised learning is novel.


{\small
\setlength{\bibsep}{0pt}
\bibliographystyle{abbrv}
\bibliography{references}
}

% \newpage
% \input{rebound}

\end{document}




