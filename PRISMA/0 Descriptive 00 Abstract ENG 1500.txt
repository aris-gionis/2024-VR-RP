Clustering is a fundamental task in data analysis, seeking to partition data into groups based on their similarity. Clustering algorithms enable scientists and practioners to extract meaningful insights from data and make informed decisions. Although largely popular, traditional clustering methods often lack transparency in explaining why certain data points are grouped together. The situation is exacerbated by opaque methods for clustering, such as spectral techniques, self-organizing maps, etc.

With the explosive growth of AI and the wide adoption of machine learning methods, there is strong demand for algorithms that provide explanations on their decisions, or employ easy-to-interpret models, and rightfully, the machine-learning research community has focused on developing transparent and interpretable models. Much of research has been devoted on explainable and interpretable supervised machine-learning methods, while significantly less attention has been given on the topic of explainability in unsupervised machine learning, such as data clustering. 

In this project we will advance the area of explainable clustering. We will develop novel theoretical frameworks for introducing transparency and explainability into the data-clustering task. We will develop explainable clustering methods that employ novel white-box models, we will devise methods for providing explanations to clusterings produced by complex models, and we will study formulations that leverage covariates.